{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74071b38",
   "metadata": {},
   "source": [
    "# Bag of Words und Naive Bayes mit Scikit-Learn\n",
    "\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "basierend auf [diesem](https://sites.pitt.edu/~naraehan/presentation/Movie%20Reviews%20sentiment%20analysis%20with%20Scikit-Learn.html) und [diesem](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#working-with-text-data) und [diesem](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) Tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "#!pip install nltk\n",
    "#!pip install matplotlib\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ba8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a5060",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc8323",
   "metadata": {},
   "source": [
    "Unser Beispiel-Korpus enthält 4 Dokumente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ad344",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54329e64",
   "metadata": {},
   "source": [
    "Die Klasse `CountVectorizer` erstellt ein *Bag of Words* aus den Dokumenten. Wir erzeugen aus der Klasse zunächst ein Objekt, das wir `vectorizer` nennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc9074",
   "metadata": {},
   "source": [
    "Bei der folgenden Methode `fit_transform()` passieren zwei Schritte:\n",
    "1. `fit()` : es wird an die Daten \"angepasst\"\n",
    "2. `transform()`: die Daten werden entsprechend transformiert\n",
    "\n",
    "(Diese Methdoen kann man auch separat aufrufen, mit `fit_transform()` ist es aber effizienter). Der Output ist eine *sparse matrix*, das ist eine Form von Matrix, die darauf optimiert wurde, eine Matrix, die hauptsächlich Nullen enthält, effizienter zu repräsentieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeabdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir speichern das hier nur in einer Variablen um später nochmal \"reinschauen\" zu können\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475a1f7",
   "metadata": {},
   "source": [
    "Wir können uns aber auch die \"echte\" Matrix (die nicht sparse ist) anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60209de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410c918",
   "metadata": {},
   "source": [
    "Was sehen wir da? Jede Zeile entspricht einem Dokument. Jede Spalte entspricht einem Feature. Und was unsere Features sind, können wir uns folgendermaßen ausgeben lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846a31f",
   "metadata": {},
   "source": [
    "Nun visualisieren wir das ganze noch etwas hübscher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X.toarray(),\n",
    "                 columns = vectorizer.get_feature_names_out(), \n",
    "                 index = corpus)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979bd16a",
   "metadata": {},
   "source": [
    "Jeder *Worttype* wird also intern auf einen Index abgebildet. Das können wir uns mit dem Attribut `vocabulary_`ausgeben lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a85b0",
   "metadata": {},
   "source": [
    "Um aus einem neuen Dokument (z.B. Testdaten) die entsprechenden Features zu extrahieren, rufen wir die Methode `transform()`(ohne `fit()`!) auf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform(['And another document.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5314e7",
   "metadata": {},
   "source": [
    "Besteht ein neues Dokument ausschließlich aus Wörtern, die in den Trainingsdaten nicht vorkamen, erhalten wir einen Nullvektor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2371cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5ed57",
   "metadata": {},
   "source": [
    "### N-Gramme\n",
    "Statt einem *Bag of Words* mit Unigrammen, können auch N-Gramme beliebiger Größe extrahiert werden. Dazu wird in `CountVectorizer` der Parameter `ngram_range` gesetzt. Das Tupel `(1,2)` bedeutet dass Uni- bis Bigramme extrahiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284165ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(bigram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e925a9",
   "metadata": {},
   "source": [
    "### Character N-Gramme\n",
    "Es können auch N-Gramme auf Zeichenbasis (characters) extrahiert werden. Dazu wird der Parameter `analyzer` mit dem Wert `char` oder `char_wb` belegt. Letzeres erstellt N-Gramme nur innerhalb der Wortgrenzen (mit Spaces als Padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f526f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_bigram_vectorizer = CountVectorizer(ngram_range=(1,2), analyzer='char_wb')\n",
    "character_bigram_vectorizer.fit_transform(corpus)\n",
    "print(character_bigram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45726a0",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for Movie Reviews with Naive Bayes and Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45125dd6",
   "metadata": {},
   "source": [
    "Mit der Funktion [`load_files()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html) können Datensätze in einem bestimmten Format sehr einfach eingelesen werden: Alle Daten, die zu einer Klasse (z. B. *positiv* vs. *negativ*) gehören, müssen in einem eigenen Unterordner liegen. Das ist z.B. beim nltk_movie_reviews Datensatz der Fall (Quelle: https://www.nltk.org/nltk_data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c50997",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = load_files(\"nltk_movie_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51613ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenstruktur betrachten\n",
    "\n",
    "# movie.data : Liste der Reviews\n",
    "# movie.target : Liste der zugehörigen Klassen (0 vs. 1)\n",
    "# movie.target_names : Liste aller Klassen\n",
    "\n",
    "print(\"Text:\", movie.data[0][:100], \"...\")\n",
    "print(\"Klasse:\", movie.target[0])\n",
    "print(\"Mögliche Klassen:\")\n",
    "for i, klasse in enumerate(movie.target_names):\n",
    "    print(i, \":\", klasse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71541f8f",
   "metadata": {},
   "source": [
    "### Daten in Trainings- und Testdaten aufsplitten\n",
    "mit `test_size` (alternativ `train_size`) geben wir an, welcher Anteil der Daten als Testdaten (bzw. Trainingsdaten) verwendet werden soll. Um die absolute Anzahl der Datenpunkte anzugeben, einfach einen Integer-Wert verwenden. Mit `shuffle = True` werden die Datenpunkte zunächst zufällig durcheinandergewürfelt. Damit wir bei jedem Durchgang dasselbe Ergebnis erhalten, muss unbedingt ein `random_state` gesetzt werden (der Wert ist egal!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd95352",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(movie.data, movie.target, shuffle=True,\n",
    "                                                          test_size = 0.20, random_state = 12)\n",
    "\n",
    "print(\"Anzahl Trainingsdaten:\", len(docs_train))\n",
    "print(\"Anzahl Testdaten:\", len(docs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f055e56",
   "metadata": {},
   "source": [
    "### Features extrahieren\n",
    "Wir verwenden hier einen einfachen *Bag of Words* mit folgenden zusätzlichen Parametern: Nur die 3.000 häufigsten Wörter (`max_features`) und nur diejenigen, die in mindestens zwei Dokumenten auftreten (`min_df`) werden verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ab836",
   "metadata": {},
   "outputs": [],
   "source": [
    "movieVzer= CountVectorizer(min_df=2, max_features=3000) # use top 3000 words only.\n",
    "docs_train_counts = movieVzer.fit_transform(docs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e444c2",
   "metadata": {},
   "source": [
    "### Klassifikator trainieren\n",
    "Die Klasse `MultinomialNB()` liefert uns einen Naive Bayes Klassifikator (für die Verwendung von Counts; bei binären Features würden wir einen `BernoulliNB()` verwenden). Wir verwenden ihn hier mit Default-Parametern. `fit` bedeutet nichts anderes als *trainieren*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66237ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(docs_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c586ab",
   "metadata": {},
   "source": [
    "### Klassifikator anwenden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7719ec",
   "metadata": {},
   "source": [
    "Aus den Testdaten müssen zunächst dieselben Features extrahiert werden wie aus den Trainingsdaten. Dazu wird der Bag of Words Vectorizer, der auf die Trainingsdaten angepasst wurde, nun auf die Testdaten angewandt (mit `transform()` ohne `fit()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638642c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test_counts = movieVzer.transform(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce5ffb",
   "metadata": {},
   "source": [
    "Mit `predict()` werden dann die Klassen für die Testdaten vorhergesagt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16098340",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(docs_test_counts)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58dc84",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Wir vergleichen nun `y_pred`, also die vorhergesagten Klassen, mit `y_test`, also den tatsächlichen Klassen für die Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc15191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe736e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfusionsmatrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    \n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassifikations-Report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b14bfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25cea92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
